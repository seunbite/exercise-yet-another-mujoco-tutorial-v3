{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a022f3ab-bce1-40a9-a436-2827d9a774d4",
   "metadata": {},
   "source": [
    "### Visual reasoning using `GPT4o` and 'OWLv2'\n",
    "This code uses `MuJoCo` simulation with `OWLv2` and `GPT-4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0df075-35e6-43be-ae55-298bccb0ec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import sys,mujoco,time,os,json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../package/helper/')\n",
    "sys.path.append('../package/mujoco_usage/')\n",
    "sys.path.append('../package/gpt_usage/')\n",
    "sys.path.append('../package/detection_module/')\n",
    "from mujoco_parser import *\n",
    "from utility import *\n",
    "from transformation import *\n",
    "from gpt_helper import *\n",
    "from owlv2 import *\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "plt.rc('xtick',labelsize=6); plt.rc('ytick',labelsize=6)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b019f-8aa4-45ea-920e-fe8810624245",
   "metadata": {},
   "source": [
    "#### Parse `UR5e with objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e697527-da06-4e53-9e4b-d842c739992e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mju_openResource: unknown file '/Users/sb/Downloads/workspace/yet-another-mujoco-tutorial-v3/asset/realistic_tabletop/./floor2.xml'\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "XML Error: Could not open file '/Users/sb/Downloads/workspace/yet-another-mujoco-tutorial-v3/asset/realistic_tabletop/./floor2.xml'\nElement 'include', line 22\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m xml_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../asset/realistic_tabletop/scene_table.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mMuJoCoParserClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTabletop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrel_xml_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/workspace/yet-another-mujoco-tutorial-v3/notebook/../package/mujoco_usage/mujoco_parser.py:52\u001b[0m, in \u001b[0;36mMuJoCoParserClass.__init__\u001b[0;34m(self, name, rel_xml_path, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Parse xml file\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_xml_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Downloads/workspace/yet-another-mujoco-tutorial-v3/notebook/../package/mujoco_usage/mujoco_parser.py:66\u001b[0m, in \u001b[0;36mMuJoCoParserClass._parse_xml\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Parse xml file\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_xml_path    \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_xml_path))\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel            \u001b[38;5;241m=\u001b[39m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMjModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_xml_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_xml_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata             \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mMjData(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt               \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mtimestep\n",
      "\u001b[0;31mValueError\u001b[0m: XML Error: Could not open file '/Users/sb/Downloads/workspace/yet-another-mujoco-tutorial-v3/asset/realistic_tabletop/./floor2.xml'\nElement 'include', line 22\n"
     ]
    }
   ],
   "source": [
    "xml_path = '../asset/realistic_tabletop/scene_table.xml'\n",
    "env = MuJoCoParserClass(name='Tabletop',rel_xml_path=xml_path,verbose=True)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346d955-8e5a-479f-b8f7-7e99cd4651eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint configuration\n",
    "joint_names = ['shoulder_pan_joint','shoulder_lift_joint','elbow_joint',\n",
    "               'wrist_1_joint','wrist_2_joint','wrist_3_joint']\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3679d-a7f8-4861-9da3-ccae85adfb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve IK to get the initial position \n",
    "env.reset()\n",
    "env.set_p_body(body_name='ur_base',p=np.array([0,0,0.5])) # move UR\n",
    "q_init,ik_err_stack,ik_info = solve_ik(\n",
    "    env = env,\n",
    "    joint_names_for_ik = joint_names,\n",
    "    body_name_trgt     = 'ur_camera_center',\n",
    "    q_init       = np.deg2rad([0,0,0,0,0,0]), # ik from zero pose\n",
    "    p_trgt       = np.array([0.41,0.0,1.2]),\n",
    "    R_trgt       = rpy2r(np.deg2rad([-135.22,-0.,-90])),\n",
    "    max_ik_tick  = 5000,\n",
    "    ik_err_th    = 1e-4,\n",
    "    ik_stepsize  = 0.1,\n",
    "    ik_eps       = 1e-2,\n",
    "    ik_th        = np.radians(1.0),\n",
    "    verbose      = False,\n",
    "    reset_env    = False,\n",
    "    render       = False,\n",
    "    render_every = 1,\n",
    ")\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d629de4-f1d9-46ed-8a11-8e5bc48799d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.random.seed(seed=0)\n",
    "env.reset()\n",
    "env.init_viewer(\n",
    "    transparent = False,\n",
    "    azimuth     = 105,\n",
    "    distance    = 3.12,\n",
    "    elevation   = -29,\n",
    "    lookat      = [0.39, 0.25, 0.43],\n",
    ")\n",
    "env.set_p_body(body_name='ur_base',p=np.array([0,0,0.5])) # move UR\n",
    "env.set_p_body(body_name='object_table',p=np.array([1.0,0,0])) # move table\n",
    "obj_names = env.get_body_names(prefix='obj_') # object names\n",
    "n_obj = len(obj_names)\n",
    "obj_xyzs = sample_xyzs(\n",
    "    n_obj,\n",
    "    x_range   = [0.75,1.25],\n",
    "    y_range   = [-0.4,+0.4],\n",
    "    z_range   = [0.51,0.51],\n",
    "    min_dist  = 0.1,\n",
    "    xy_margin = 0.0\n",
    ")\n",
    "print (\"Object list:\")\n",
    "for obj_idx in range(n_obj):\n",
    "    print (\" [%d] obj_name:[%s]\"%(obj_idx,obj_names[obj_idx]))\n",
    "    env.set_p_base_body(body_name=obj_names[obj_idx],p=obj_xyzs[obj_idx,:])\n",
    "    env.set_R_base_body(body_name=obj_names[obj_idx],R=np.eye(3,3))\n",
    "# Move\n",
    "qpos = np.radians([0,-90,60,75,90,0])\n",
    "idxs_step = env.get_idxs_step(joint_names=joint_names)\n",
    "env.set_qpos_joints(joint_names=joint_names,qpos=q_init)\n",
    "# Loop\n",
    "env_state = env.get_state()\n",
    "while env.is_viewer_alive():\n",
    "    # Step\n",
    "    env.step(ctrl=q_init,ctrl_idxs=idxs_step)\n",
    "    # Grab RGB-D \n",
    "    if env.loop_every(tick_every=10):\n",
    "        p_cam,R_cam = env.get_pR_body(body_name='ur_camera_center')\n",
    "        p_ego  = p_cam\n",
    "        p_trgt = p_cam + R_cam[:,2] # z-axis forward\n",
    "        ego_rgb_img,ego_depth_img,pcd,xyz_img,xyz_img_world = env.get_egocentric_rgbd_pcd(\n",
    "            p_ego            = p_ego,\n",
    "            p_trgt           = p_trgt,\n",
    "            rsz_rate_for_pcd = 1/20, # 1/4\n",
    "            rsz_rate_for_img = 1/4,\n",
    "            fovy             = 45, # env.model.cam_fovy[0]\n",
    "            restore_view     = True,\n",
    "        )\n",
    "    # Render\n",
    "    if env.loop_every(tick_every=10):\n",
    "        env.plot_T(p=np.array([0,0,0]),R=np.eye(3))\n",
    "        env.plot_time()\n",
    "        env.plot_body_T(body_name='ur_camera_center',axis_len=0.1,axis_width=0.005)\n",
    "        env.render()\n",
    "    # Plot\n",
    "    if env.loop_every(tick_every=500): # every 1 second\n",
    "        # Grab current view\n",
    "        render_img = env.grab_image(rsz_rate=1/4)\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(render_img)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(ego_rgb_img)\n",
    "        plt.suptitle(\"Time:[%.2f]sec\"%(env.get_sim_time()),fontsize=10)\n",
    "        # plt.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1, top=0.99)\n",
    "        plt.show()\n",
    "env.close_viewer()\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c100514-5d6c-48d3-9793-ec530d46aa1a",
   "metadata": {},
   "source": [
    "#### Save the egocentric view `ego_rgb_img` as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f762cd-ddaa-4148-9d7e-00f84fc0c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path = './.tmp/mujoco_objects.png'\n",
    "save_png(ego_rgb_img,png_path,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968b3ac-a5f2-4995-ae8f-05fa258877ba",
   "metadata": {},
   "source": [
    "#### Instantiate `GPT4o` agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca4b3b-8fa7-4a6d-945a-57871980e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GPT4V helper\n",
    "GPT = GPT4VchatClass(\n",
    "    gpt_model = \"gpt-4o\", #\"gpt-4-vision-preview\",\n",
    "    role_msg  = \"You are a helpful agent with vision capabilities; do not respond to objects not depicted in images.\",\n",
    "    key_path  = \"../key/rilab_key.txt\",\n",
    ")\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1c377-be98-40fd-bd1c-cd276bfde47e",
   "metadata": {},
   "source": [
    "#### Ask `GPT4o` to describe the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b85224-0c8b-46fb-b198-0c25e7922003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(ego_rgb_img)\n",
    "plt.show()\n",
    "# Describe the image\n",
    "GPT.chat(\n",
    "    user_msg = \"<img1>Describe the image.\",\n",
    "    images   = [png_path],\n",
    "    PRINT_USER_MSG   = True,\n",
    "    PRINT_GPT_OUTPUT = True,\n",
    "    RESET_CHAT       = False,\n",
    "    RETURN_RESPONSE  = False,\n",
    "    MAX_TOKENS       = 512,\n",
    ")\n",
    "# List down the objects in the scene\n",
    "ret = GPT.chat(\n",
    "    user_msg = \"\"\"\n",
    "        Could you list down the object names suitable for detection in a json format? \n",
    "        Below is an example of a json format:\n",
    "        {\n",
    "          \"objects\": [\n",
    "            \"apple fruit\",\n",
    "            \"lemon fruit\",\n",
    "            \"orange fruit\",\n",
    "            \"coke can\",\n",
    "            \"kitcat cookie\",\n",
    "          ]\n",
    "        }\n",
    "        \"\"\",\n",
    "    images   = None,\n",
    "    PRINT_USER_MSG   = True,\n",
    "    PRINT_GPT_OUTPUT = True,\n",
    "    RESET_CHAT       = False,\n",
    "    RETURN_RESPONSE  = True,\n",
    "    MAX_TOKENS       = 512,\n",
    ")\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb8c7b-edd9-4706-8508-9c8fb88bc4b2",
   "metadata": {},
   "source": [
    "#### Run open-vocab detection (i.e., `Owlv2`) on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce48947-0dd4-4628-826e-b6253a1ba10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse object names\n",
    "json_string = ret[ret.find('{'):ret.rfind('}')+1]\n",
    "data = json.loads(json_string)\n",
    "object_names = data['objects']\n",
    "print (\"object_names:%s\"%(object_names))\n",
    "\n",
    "# Open-vocab detection\n",
    "owlvit = Owlv2()\n",
    "detection_result = owlvit.detect_objects(\n",
    "    image_path    = png_path,\n",
    "    object_names  = object_names,\n",
    "    box_threshold = 0.25,\n",
    ")\n",
    "\n",
    "# Plot detection results\n",
    "plot_detection_result(\n",
    "    image_path       = png_path,\n",
    "    detection_result = detection_result,\n",
    "    figsize          = (6,4),\n",
    "    fontsize         = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e5c87-96de-4d1f-8d62-69e7abdaf8d5",
   "metadata": {},
   "source": [
    "#### Get non-overlapping bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18377b-cb85-4f80-8da3-b9aeecedc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcd xyzs of objects\n",
    "xyz_bboxes = []\n",
    "for box in detection_result['boxes']: # for each detection boxes\n",
    "    # First get xyz corresponding to the bbox\n",
    "    x1,y1,x2,y2 = (box*xyz_img_world.shape[0]/ego_rgb_img.shape[0]).int()\n",
    "    xyz = xyz_img_world[y1:y2+1,x1:x2+1,:].reshape((-1,3)) # [M x 3]\n",
    "    xyz_bboxes.append(xyz) # append pcd xyz of bbox\n",
    "# Exclude overlapping pcd\n",
    "xyz_bboxes_unique = exclude_overlapping_pcd_within_list(xyz_bboxes)\n",
    "# Get pcd xyz of objects and center of the objects    \n",
    "xyz_objects,xyz_centers = [],[]\n",
    "for xyz in xyz_bboxes_unique:\n",
    "    # Get xyz of an object excluding the floor\n",
    "    z_discretized = np.round(xyz[:,2]/0.01)*0.01 # discretized height [M]\n",
    "    unique,counts = np.unique(z_discretized,return_counts=True)\n",
    "    k = int(len(unique)*0.5) # for robust floor detection\n",
    "    top_k_indices = np.argsort(counts)[-k:]\n",
    "    top_k_unique_values = unique[top_k_indices]\n",
    "    z_floor = np.min(top_k_unique_values)+0.005 # get z floor\n",
    "    xyz_object = xyz[xyz[:,2] >= (z_floor)] # get xyz above the floor\n",
    "    xyz_objects.append(xyz_object) # append pcd xyz above the floor\n",
    "    # Now, get the center of the object in a xy plane\n",
    "    xy_object = xyz_object[:,:2] # [M x 2]\n",
    "    xy_center,radius = fit_xy_circle(xy_object) # center:[2]\n",
    "    xyz_centers.append(np.append(xy_center,z_floor)) # [3]\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68411da-a4d3-44da-9938-f864e91f184e",
   "metadata": {},
   "source": [
    "#### Get object center points using point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd2962-e060-4b43-b7db-8d9e0884b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.set_state(**env_state)\n",
    "env.init_viewer(azimuth=170,distance=2.3,elevation=-35,lookat=[0.01,0.1,-0.25],\n",
    "                transparent=True,maxgeom=100000)\n",
    "while env.is_viewer_alive():\n",
    "    # Step\n",
    "    env.step(ctrl=q_init,ctrl_idxs=idxs_step)\n",
    "    # Render\n",
    "    if env.loop_every(tick_every=10):\n",
    "        for idx in range(detection_result['n']):\n",
    "            env.plot_spheres(ps=xyz_bboxes[idx],r=0.002,rgba=(1,0,0,0.5))\n",
    "            env.plot_spheres(ps=xyz_objects[idx],r=0.0025,rgba=(0,0,1,0.5))\n",
    "            env.plot_T(p=xyz_centers[idx],R=np.eye(3),plot_axis=True,axis_len=0.15,axis_width=0.005)\n",
    "            object_name = detection_result['object_names'][detection_result['labels'][idx]]\n",
    "            env.plot_text(p=xyz_centers[idx]+np.array([0,0,0.15]),label='%s'%(object_name))\n",
    "        env.render()\n",
    "    # Plot\n",
    "    if env.loop_every(tick_every=5000): # every 10 second\n",
    "        # Grab current view\n",
    "        render_img = env.grab_image(rsz_rate=1)\n",
    "        # Plot\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.imshow(render_img)\n",
    "        plt.title(\"Time:[%.2f]sec\"%(env.get_sim_time()),fontsize=10)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "env.close_viewer()\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc560b-c5e6-4321-90b5-850aaec55928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store current GPT message\n",
    "gpt_state = GPT.get_state()\n",
    "print (\"GPT state ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38580396-1a31-4187-b82d-1a11ecaf54a2",
   "metadata": {},
   "source": [
    "#### Little bit of `common-sense` inference using `GPT4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad49e47-b250-4939-8b9c-f68f879aed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPT state\n",
    "GPT.set_state(state=gpt_state)\n",
    "ret = GPT.chat(\n",
    "    user_msg = \"\"\"\n",
    "        Could you recommend one thing on the table that I can eat?\n",
    "        BTW, I am very tired. It would be better if you can give me reasons as well.\n",
    "        \"\"\",\n",
    "    images   = None,\n",
    "    PRINT_USER_MSG   = True,\n",
    "    PRINT_GPT_OUTPUT = True,\n",
    "    RESET_CHAT       = False,\n",
    "    RETURN_RESPONSE  = True,\n",
    "    MAX_TOKENS       = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae0f8e-b17a-492f-9280-0eaa526eebbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
